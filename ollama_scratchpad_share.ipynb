{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aa4294d",
   "metadata": {},
   "source": [
    "## Let's Talk Digital: synergistic human-LLM deductive coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e169af96",
   "metadata": {},
   "source": [
    "_WIP - NOT FOR DISTRIBUTION_\n",
    "\n",
    "_Proof-of-concept adaptation of the [CHALET](https://arxiv.org/abs/2405.05758) (**C**ollaborative **H**uman-LLM **A**na**L**ysis for **E**mpowering Conceptualization in Quali**T**ative Research) approach for Let's Talk Digital acceptability, feasibility, and usability interview data._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fdfafe",
   "metadata": {},
   "source": [
    "> ollama_scratchpad.ipynb<br> \n",
    "> Simone J. Skeen (01-02-2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccf1cf8",
   "metadata": {},
   "source": [
    "### Prepare\n",
    "***\n",
    "Installs, imports, requisite packages; customizes outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0082bddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install python-docx\n",
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dce19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import json\n",
    "import ollama\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "pd.set_option(\n",
    "              'display.max_columns',\n",
    "              None,\n",
    "              )\n",
    "\n",
    "pd.set_option(\n",
    "              'display.max_rows',\n",
    "              None,\n",
    "              )\n",
    "\n",
    "warnings.simplefilter(\n",
    "                      action = 'ignore',\n",
    "                      category = FutureWarning,\n",
    "                      )\n",
    "\n",
    "#from langchain_community.llms import Ollama "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d8b331",
   "metadata": {},
   "source": [
    "### Write\n",
    "***\n",
    "Defines parse_interview_responses and code_interview_responses functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077effc5",
   "metadata": {},
   "source": [
    "**_parse_interview_responses_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5299e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "        ### SJS 12/31: _NOTE_ manually removing interview questions; zero-shot Llama in future\n",
    "\n",
    "def parse_interview_responses(docx_file):\n",
    "    \"\"\"\n",
    "    Reads a .docx interview transcription, extracts each distinct response (denoted 'R:'),\n",
    "    returns pandas df with columns: ['text', '<tag>_1...n'].\n",
    "    \"\"\"\n",
    "\n",
    "    # load .docx file\n",
    "    \n",
    "    doc = docx.Document(docx_file)\n",
    "\n",
    "    # concatenate responses into single string\n",
    "    \n",
    "    full_text = []\n",
    "    for p in doc.paragraphs:\n",
    "        full_text.append(p.text)\n",
    "    all_text = '\\n'.join(full_text)\n",
    "\n",
    "    # regex to capture distinct responses starting 'R:' until the next 'R:' or EOF\n",
    "    \n",
    "        ### SJS 1/1: GPT-o1-derived regex; verify\n",
    " \n",
    "    #  - (R:.*?): Capture 'R:' followed by any characters, non-greedily (.*?)\n",
    "    #  - (?=\\n\\s*R:|\\Z): Look ahead for a newline+whitespace+R: OR the end of the string (\\Z)\n",
    "    pattern = r'(R:.*?)(?=\\n\\s*R:|\\Z)'\n",
    "\n",
    "    responses = re.findall(pattern, all_text, flags = re.DOTALL)\n",
    "\n",
    "    # strip trailing whitespaces and linebreaks from each response\n",
    "    \n",
    "    responses = [resp.strip() for resp in responses]\n",
    "\n",
    "    # 5) build df\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'text': responses,\n",
    "        'comm_sjs': None ### 'comm_sjs' = enhancing communications tag; add as needed\n",
    "        # <tag n> etc.\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7adabf0",
   "metadata": {},
   "source": [
    "**_code_interview_responses_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf44dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_interview_responses(df, text_column, endpoint_url, prompt_template, model_name):\n",
    "    \n",
    "        ### SJS 1/1: _NOTE_ enhancing communication tag used as proof of concept currently; translate with f-string TKTK\n",
    "    \n",
    "    \"\"\"\n",
    "    Classifies each row of 'text' column in provided df in accord with human-specified prompt,\n",
    "    includes chain-of-thought reasoning, returning explanations for classification decision.\n",
    " \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame containing the text to classify.\n",
    "    text_column : str\n",
    "        The column name in df containing the text to be analyzed.\n",
    "    endpoint_url : str\n",
    "        The URL where locally hosted Llama model runs.\n",
    "    prompt_template : str\n",
    "        The prompt text with a placeholder (e.g., '{text}') where the row's text will be inserted.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        The original DataFrame with two new columns: 'comm_llm' (either \"0\" or \"1\")\n",
    "        and 'comm_expl' (the explanation).\n",
    "    \"\"\"\n",
    "\n",
    "    # create empty tag ['*_llm'] and reasoning ['*_expl'] column\n",
    "    \n",
    "    df['comm_llm'] = None\n",
    "    df['comm_expl'] = None\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        row_text = row[text_column]\n",
    "\n",
    "        # replace '{text}' in prompt_template with df 'text' data\n",
    "        \n",
    "        prompt = prompt_template.format(text = row_text)\n",
    "\n",
    "        # send request to local Llama endpoint.\n",
    "        \n",
    "        response = requests.post(\n",
    "            endpoint_url,\n",
    "            headers = {'Content-Type': 'application/json'},\n",
    "            json = {\n",
    "                'model': model_name,\n",
    "                'prompt': prompt,\n",
    "                'stream': False \n",
    "                },\n",
    "        )\n",
    "\n",
    "        # print statements for debugging\n",
    "        \n",
    "        print(response.status_code)\n",
    "        print(response.text)      \n",
    "\n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                # Step 1: parse top-level JSON\n",
    "                \n",
    "                result_json = response.json()\n",
    "                \n",
    "                # Step 2: the 'response' field contains the JSON string with comm_llm & comm_expl\n",
    "                \n",
    "                raw_response_str = result_json.get('response', ' ')\n",
    "                \n",
    "                # Step 3: parse inner JSON\n",
    "                \n",
    "                parsed_output = json.loads(raw_response_str)\n",
    "                \n",
    "                # Step 4: extract tag and reasoning fields\n",
    "                \n",
    "                comm_llm_label = parsed_output.get('comm_llm')\n",
    "                comm_llm_expl  = parsed_output.get('comm_expl')\n",
    "                \n",
    "            except (json.JSONDecodeError, KeyError, TypeError) as e:\n",
    "                print(\"Parsing error:\", e)\n",
    "                comm_llm_label = None\n",
    "                comm_llm_expl = None        \n",
    "           \n",
    "        #if response.status_code == 200:\n",
    "        #    try:\n",
    "        #        # Expecting the server's response to be valid JSON containing comm_llm and comm_expl\n",
    "        #        result_json = response.json()\n",
    "        #        comm_llm_label = result_json.get(\"comm_llm\", None)\n",
    "        #        comm_llm_expl = result_json.get(\"comm_expl\", None)\n",
    "        #    except ValueError:\n",
    "        #        # If JSON parsing fails, set these fields to None or provide fallback values\n",
    "        #        comm_llm_label = None\n",
    "        #        comm_llm_expl = None\n",
    "        \n",
    "        # 'None' if lacking valid status code\n",
    "        \n",
    "        else:\n",
    "            comm_llm_label = None\n",
    "            comm_llm_expl = None\n",
    "\n",
    "        # insert classification results into df\n",
    "        \n",
    "        df.at[idx, 'comm_llm'] = comm_llm_label\n",
    "        df.at[idx, 'comm_expl'] = comm_llm_expl\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e0f68a",
   "metadata": {},
   "source": [
    "### Transform\n",
    "***\n",
    "Imports raw interview data, parses via regex, transforms into structured pandas dataframe/Excel .xlsx file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3b75f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set wd\n",
    "\n",
    "#%pwd\n",
    "os.chdir('C:/Users/sskee/OneDrive/Desktop/ltd_sham')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39413dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matrix - sham data\n",
    "\n",
    "#d_sham = pd.read_excel(\n",
    "#                       'ltd_qual_sham.xlsx',\n",
    "                       #index_col = [0],\n",
    "#                       )\n",
    "\n",
    "# inspect\n",
    "\n",
    "#print(d_sham.columns)\n",
    "#d_sham.info()\n",
    "#d_sham.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e8279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import interview transcript - c10 as pilot\n",
    "\n",
    "        ### SJS 1/1: for t in transcripts loop TKTK\n",
    "\n",
    "c10 = parse_interview_responses(\"Caregiver_10.docx\")\n",
    "c10.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ee10a4-294d-44b9-b06d-6c6173c12f55",
   "metadata": {},
   "source": [
    "### Code\n",
    "***\n",
    "Enables human-LLM deductive coding: human-specified per-tag prompts, JSON-.xlsx structured outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397ff6e7-b1e0-4e9f-819f-d14143b0ef3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Enhancing communication skills (alias: `comm`): prompt formulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595eb347-29ce-4ffe-93b1-0fc1c6be1975",
   "metadata": {},
   "outputs": [],
   "source": [
    "        ### SJS 1/2: _NOTE_ prelim sample\n",
    "\n",
    "role = '''\n",
    "You are tasked with applying pre-defined qualitative codes to segments of text excerpted from interviews with\n",
    "graduates of a family-strengthening HIV prevention intervention. You will be provided a definition, instructions, \n",
    "and key exemplars of text to guide your coding decisions.\n",
    "'''\n",
    "\n",
    "definition = '''\n",
    "Definition of \"Enhancing communication skills\": any description of the ability to speak and converse clearly, \n",
    "candidly, patiently, and receptively, without resorting to anger or assumptions, with family members.\n",
    "'''\n",
    "\n",
    "instruction = '''\n",
    "You will be provided with a piece of text. For each piece of text:\n",
    "- If it meets the definition of \"Enhancing communication skills,\" output comm_llm as \"1\".\n",
    "- Otherwise, output comm_llm as \"0\".\n",
    "- Also provide a short explanation in exactly two sentences, stored in comm_expl.\n",
    "\n",
    "Please respond in valid JSON with keys \"comm_llm\" and \"comm_expl\" only.\n",
    "\n",
    "Text:\n",
    "{text}\n",
    "'''\n",
    "\n",
    "examples = '''\n",
    "Below are human-validated examples of \"Enhancing communication skills\"\n",
    "\n",
    "- \"<example_1>.\"\n",
    "      \n",
    "- \"<example_2>.\"\n",
    "      \n",
    "- \"<example_3>.\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1063fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate prompt as f-string\n",
    "\n",
    "comm_prompt = f'{role}{definition}{instruction}{examples}'\n",
    "\n",
    "# locally hosted Llama endpoint\n",
    "\n",
    "llama_endpoint = 'http://localhost:11434/api/generate'\n",
    "\n",
    "# classify texts and update df\n",
    "\n",
    "df = code_interview_responses(\n",
    "    c10,\n",
    "    text_column = 'text',\n",
    "    endpoint_url = llama_endpoint,\n",
    "    prompt_template = comm_prompt,\n",
    "    model_name = 'llama3',\n",
    ")\n",
    "\n",
    "#print(df)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345b6966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "df.to_excel('c10_prelim.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a02515-b857-4b38-b55b-ac5253a95e6d",
   "metadata": {},
   "source": [
    "> End of ollama_scratchpad.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
